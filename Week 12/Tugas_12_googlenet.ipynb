{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/putriifeb06/ML-TK-42-ANDI-PUTRI-FEBRIANTI/blob/main/Week%2012/Tugas_12_googlenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Networks with Parallel Concatenations (GoogLeNet)**\n",
        "Nama: Andi Putri Febrianti \n",
        "\n",
        "NIM: 1103174147 \n",
        "\n",
        "Kelas: TK-42-02 \n",
        "\n",
        "Sumber: http://www.d2l.ai/chapter_convolutional-modern/googlenet.html?highlight=googlenet"
      ],
      "metadata": {
        "id": "VznjlN-fP8RL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dwb7fRMaP5Su",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77f805be-87c7-4dc9-fe32-1528b04f7b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d2l==0.17.5\n",
            "  Downloading d2l-0.17.5-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 399 kB/s \n",
            "\u001b[?25hCollecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.7/dist-packages (from d2l==0.17.5) (1.0.0)\n",
            "Collecting pandas==1.2.4\n",
            "  Downloading pandas-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 33.2 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.5.1\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 39.3 MB/s \n",
            "\u001b[?25hCollecting numpy==1.21.5\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 53.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l==0.17.5) (5.3.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l==0.17.5) (7.7.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l==0.17.5) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l==0.17.5) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l==0.17.5) (4.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l==0.17.5) (5.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l==0.17.5) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l==0.17.5) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l==0.17.5) (21.3)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
            "\u001b[K     |████████████████████████████████| 930 kB 63.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l==0.17.5) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l==0.17.5) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l==0.17.5) (1.4.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->d2l==0.17.5) (2022.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l==0.17.5) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l==0.17.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l==0.17.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l==0.17.5) (2022.6.15)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.5.1->d2l==0.17.5) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->d2l==0.17.5) (1.15.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l==0.17.5) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l==0.17.5) (5.3.5)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l==0.17.5) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l==0.17.5) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l==0.17.5) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l==0.17.5) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l==0.17.5) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l==0.17.5) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l==0.17.5) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l==0.17.5) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l==0.17.5) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l==0.17.5) (0.2.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==0.17.5) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==0.17.5) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==0.17.5) (1.1.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==0.17.5) (5.4.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l==0.17.5) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l==0.17.5) (2.15.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l==0.17.5) (4.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l==0.17.5) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l==0.17.5) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l==0.17.5) (4.11.4)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l==0.17.5) (5.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l==0.17.5) (3.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l==0.17.5) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l==0.17.5) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l==0.17.5) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter==1.0.0->d2l==0.17.5) (23.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter==1.0.0->d2l==0.17.5) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter==1.0.0->d2l==0.17.5) (2.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l==0.17.5) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l==0.17.5) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l==0.17.5) (0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l==0.17.5) (5.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l==0.17.5) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l==0.17.5) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l==0.17.5) (0.5.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter==1.0.0->d2l==0.17.5) (2.1.0)\n",
            "Installing collected packages: numpy, fonttools, requests, pandas, matplotlib, d2l\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.25.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed d2l-0.17.5 fonttools-4.33.3 matplotlib-3.5.1 numpy-1.21.5 pandas-1.2.4 requests-2.25.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet-cu101==1.7.0\n",
            "  Downloading mxnet_cu101-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (846.0 MB)\n",
            "\u001b[K     |███████████████████████████████▌| 834.1 MB 1.2 MB/s eta 0:00:10tcmalloc: large alloc 1147494400 bytes == 0x3a6d8000 @  0x7ffbd0ca5615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 846.0 MB 21 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101==1.7.0) (2.25.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101==1.7.0) (1.21.5)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2022.6.15)\n",
            "Installing collected packages: graphviz, mxnet-cu101\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101-1.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l==0.17.5\n",
        "!pip install -U mxnet-cu101==1.7.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "_q7mSHg-P5Sx"
      },
      "source": [
        "# Networks with Parallel Concatenations (GoogLeNet)\n",
        ":label:`sec_googlenet`\n",
        "\n",
        "In 2014, *GoogLeNet*\n",
        "won the ImageNet Challenge, proposing a structure\n",
        "that combined the strengths of NiN and  paradigms of repeated blocks :cite:`Szegedy.Liu.Jia.ea.2015`.\n",
        "One focus of the paper was to address the question\n",
        "of which sized convolution kernels are best.\n",
        "After all, previous popular networks employed choices\n",
        "as small as $1 \\times 1$ and as large as $11 \\times 11$.\n",
        "One insight in this paper was that sometimes\n",
        "it can be advantageous to employ a combination of variously-sized kernels.\n",
        "In this section, we will introduce GoogLeNet,\n",
        "presenting a slightly simplified version of the original model:\n",
        "we\n",
        "omit a few ad-hoc features that were added to stabilize training\n",
        "but are unnecessary now with better training algorithms available.\n",
        "\n",
        "\n",
        "## (**Inception Blocks**)\n",
        "\n",
        "The basic convolutional block in GoogLeNet is called an *Inception block*,\n",
        "likely named due to a quote from the movie *Inception* (\"We need to go deeper\"),\n",
        "which launched a viral meme.\n",
        "\n",
        "![Structure of the Inception block.](http://d2l.ai/_images/inception.svg)\n",
        ":label:`fig_inception`\n",
        "\n",
        "As depicted in :numref:`fig_inception`,\n",
        "the inception block consists of four parallel paths.\n",
        "The first three paths use convolutional layers\n",
        "with window sizes of $1\\times 1$, $3\\times 3$, and $5\\times 5$\n",
        "to extract information from different spatial sizes.\n",
        "The middle two paths perform a $1\\times 1$ convolution on the input\n",
        "to reduce the number of channels, reducing the model's complexity.\n",
        "The fourth path uses a $3\\times 3$ maximum pooling layer,\n",
        "followed by a $1\\times 1$ convolutional layer\n",
        "to change the number of channels.\n",
        "The four paths all use appropriate padding to give the input and output the same height and width.\n",
        "Finally, the outputs along each path are concatenated\n",
        "along the channel dimension and comprise the block's output.\n",
        "The commonly-tuned hyperparameters of the Inception block\n",
        "are the number of output channels per layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "origin_pos": 1,
        "tab": [
          "mxnet"
        ],
        "id": "__mHxuCaP5S0"
      },
      "outputs": [],
      "source": [
        "from mxnet import np, npx\n",
        "from mxnet.gluon import nn\n",
        "from d2l import mxnet as d2l\n",
        "\n",
        "npx.set_np()\n",
        "\n",
        "class Inception(nn.Block):\n",
        "    # `c1`--`c4` are the number of output channels for each path\n",
        "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
        "        super(Inception, self).__init__(**kwargs)\n",
        "        # Path 1 is a single 1 x 1 convolutional layer\n",
        "        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n",
        "        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3\n",
        "        # convolutional layer\n",
        "        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
        "        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,\n",
        "                              activation='relu')\n",
        "        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5\n",
        "        # convolutional layer\n",
        "        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n",
        "        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,\n",
        "                              activation='relu')\n",
        "        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\n",
        "        # convolutional layer\n",
        "        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n",
        "        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        p1 = self.p1_1(x)\n",
        "        p2 = self.p2_2(self.p2_1(x))\n",
        "        p3 = self.p3_2(self.p3_1(x))\n",
        "        p4 = self.p4_2(self.p4_1(x))\n",
        "        # Concatenate the outputs on the channel dimension\n",
        "        return np.concatenate((p1, p2, p3, p4), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "xAZjqVLVP5S2"
      },
      "source": [
        "To gain some intuition for why this network works so well,\n",
        "consider the combination of the filters.\n",
        "They explore the image in a variety of filter sizes.\n",
        "This means that details at different extents\n",
        "can be recognized efficiently by filters of different sizes.\n",
        "At the same time, we can allocate different amounts of parameters\n",
        "for different filters.\n",
        "\n",
        "\n",
        "## [**GoogLeNet Model**]\n",
        "\n",
        "As shown in :numref:`fig_inception_full`, GoogLeNet uses a stack of a total of 9 inception blocks\n",
        "and global average pooling to generate its estimates.\n",
        "Maximum pooling between inception blocks reduces the dimensionality.\n",
        "The first module is similar to AlexNet and LeNet.\n",
        "The stack of blocks is inherited from VGG\n",
        "and the global average pooling avoids\n",
        "a stack of fully-connected layers at the end.\n",
        "\n",
        "![The GoogLeNet architecture.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/inception-full.svg?raw=1)\n",
        ":label:`fig_inception_full`\n",
        "\n",
        "We can now implement GoogLeNet piece by piece.\n",
        "The first module uses a 64-channel $7\\times 7$ convolutional layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "origin_pos": 5,
        "tab": [
          "mxnet"
        ],
        "id": "_iBJ6puIP5S3"
      },
      "outputs": [],
      "source": [
        "b1 = nn.Sequential()\n",
        "b1.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),\n",
        "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "rgl0ZJ8FP5S4"
      },
      "source": [
        "The second module uses two convolutional layers:\n",
        "first, a 64-channel $1\\times 1$ convolutional layer,\n",
        "then a $3\\times 3$ convolutional layer that triples the number of channels. This corresponds to the second path in the Inception block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "origin_pos": 9,
        "tab": [
          "mxnet"
        ],
        "id": "xfBKwuJ1P5S5"
      },
      "outputs": [],
      "source": [
        "b2 = nn.Sequential()\n",
        "b2.add(nn.Conv2D(64, kernel_size=1, activation='relu'),\n",
        "       nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),\n",
        "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "LWHpDMKyP5S5"
      },
      "source": [
        "The third module connects two complete Inception blocks in series.\n",
        "The number of output channels of the first Inception block is\n",
        "$64+128+32+32=256$,\n",
        "and the number-of-output-channel ratio\n",
        "among the four paths is $64:128:32:32=2:4:1:1$.\n",
        "The second and third paths first reduce the number of input channels\n",
        "to $96/192=1/2$ and $16/192=1/12$, respectively,\n",
        "and then connect the second convolutional layer.\n",
        "The number of output channels of the second Inception block\n",
        "is increased to $128+192+96+64=480$, and the number-of-output-channel ratio\n",
        "among the four paths is $128:192:96:64 = 4:6:3:2$.\n",
        "The second and third paths first reduce the number of input channels\n",
        "to $128/256=1/2$ and $32/256=1/8$, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "origin_pos": 13,
        "tab": [
          "mxnet"
        ],
        "id": "6oCRnbuyP5S6"
      },
      "outputs": [],
      "source": [
        "b3 = nn.Sequential()\n",
        "b3.add(Inception(64, (96, 128), (16, 32), 32),\n",
        "       Inception(128, (128, 192), (32, 96), 64),\n",
        "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "id": "LbaKDo9CP5S7"
      },
      "source": [
        "The fourth module is more complicated.\n",
        "It connects five Inception blocks in series,\n",
        "and they have $192+208+48+64=512$, $160+224+64+64=512$,\n",
        "$128+256+64+64=512$, $112+288+64+64=528$,\n",
        "and $256+320+128+128=832$ output channels, respectively.\n",
        "The number of channels assigned to these paths is similar\n",
        "to that in the third module:\n",
        "the second path with the $3\\times 3$ convolutional layer\n",
        "outputs the largest number of channels,\n",
        "followed by the first path with only the $1\\times 1$ convolutional layer,\n",
        "the third path with the $5\\times 5$ convolutional layer,\n",
        "and the fourth path with the $3\\times 3$ maximum pooling layer.\n",
        "The second and third paths will first reduce\n",
        "the number of channels according to the ratio.\n",
        "These ratios are slightly different in different Inception blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "origin_pos": 17,
        "tab": [
          "mxnet"
        ],
        "id": "I4ARTWGWP5TB"
      },
      "outputs": [],
      "source": [
        "b4 = nn.Sequential()\n",
        "b4.add(Inception(192, (96, 208), (16, 48), 64),\n",
        "       Inception(160, (112, 224), (24, 64), 64),\n",
        "       Inception(128, (128, 256), (24, 64), 64),\n",
        "       Inception(112, (144, 288), (32, 64), 64),\n",
        "       Inception(256, (160, 320), (32, 128), 128),\n",
        "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 20,
        "id": "u4KYqEUJP5TB"
      },
      "source": [
        "The fifth module has two Inception blocks with $256+320+128+128=832$\n",
        "and $384+384+128+128=1024$ output channels.\n",
        "The number of channels assigned to each path\n",
        "is the same as that in the third and fourth modules,\n",
        "but differs in specific values.\n",
        "It should be noted that the fifth block is followed by the output layer.\n",
        "This block uses the global average pooling layer\n",
        "to change the height and width of each channel to 1, just as in NiN.\n",
        "Finally, we turn the output into a two-dimensional array\n",
        "followed by a fully-connected layer\n",
        "whose number of outputs is the number of label classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "origin_pos": 21,
        "tab": [
          "mxnet"
        ],
        "id": "rnchlIWzP5TC"
      },
      "outputs": [],
      "source": [
        "b5 = nn.Sequential()\n",
        "b5.add(Inception(256, (160, 320), (32, 128), 128),\n",
        "       Inception(384, (192, 384), (48, 128), 128),\n",
        "       nn.GlobalAvgPool2D())\n",
        "\n",
        "net = nn.Sequential()\n",
        "net.add(b1, b2, b3, b4, b5, nn.Dense(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 24,
        "id": "z7xsmSSjP5TD"
      },
      "source": [
        "The GoogLeNet model is computationally complex,\n",
        "so it is not as easy to modify the number of channels as in VGG.\n",
        "[**To have a reasonable training time on Fashion-MNIST,\n",
        "we reduce the input height and width from 224 to 96.**]\n",
        "This simplifies the computation.\n",
        "The changes in the shape of the output\n",
        "between the various modules are demonstrated below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "origin_pos": 25,
        "tab": [
          "mxnet"
        ],
        "id": "UTaNSiBnP5TD",
        "outputId": "11048ae1-1a15-443a-9bbf-50f0e8538532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequential0 output shape:\t (1, 64, 24, 24)\n",
            "sequential1 output shape:\t (1, 192, 12, 12)\n",
            "sequential2 output shape:\t (1, 480, 6, 6)\n",
            "sequential3 output shape:\t (1, 832, 3, 3)\n",
            "sequential4 output shape:\t (1, 1024, 1, 1)\n",
            "dense0 output shape:\t (1, 10)\n"
          ]
        }
      ],
      "source": [
        "X = np.random.uniform(size=(1, 1, 96, 96))\n",
        "net.initialize()\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.name, 'output shape:\\t', X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 30,
        "id": "iEcAH__FP5TG"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* The Inception block is equivalent to a subnetwork with four paths. It extracts information in parallel through convolutional layers of different window shapes and maximum pooling layers. $1 \\times 1$ convolutions reduce channel dimensionality on a per-pixel level. Maximum pooling reduces the resolution.\n",
        "* GoogLeNet connects multiple well-designed Inception blocks with other layers in series. The ratio of the number of channels assigned in the Inception block is obtained through a large number of experiments on the ImageNet dataset.\n",
        "* GoogLeNet, as well as its succeeding versions, was one of the most efficient models on ImageNet, providing similar test accuracy with lower computational complexity.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Tugas 12 googlenet.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}